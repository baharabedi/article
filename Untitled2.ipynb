{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZ/MkO+jd9xcVuwEKL9k45",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/baharabedi/article/blob/main/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjh4ollqcn6l",
        "outputId": "f0361304-e2e6-4527-b2fa-0166fa3a4bea"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oShwDQhlcP7F",
        "outputId": "10412d0d-54a5-4a2b-a646-32f65c875f13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Dataset loaded successfully.\n",
            "Applying Target Encoding for 'BC' feature...\n",
            "Creating new ratio features...\n",
            "‚úÖ Advanced feature engineering complete.\n",
            "üìä Generating correlation plot for main features against the target...\n",
            "‚úÖ Features scaled using RobustScaler.\n",
            "üöÄ Starting feature selection with RFECV on new features...\n",
            "‚úÖ RFECV selected 16 optimal features:\n",
            " ['CON', 'UNCON', 'EP', 'PV', 'AQ', 'EL', 'SIZE', 'INST', 'LEV', 'CON_EP_interaction', 'SIZE_LEV_interaction', 'CON_div_SIZE', 'LEV_div_SIZE', 'EP_div_AQ', 'CON_div_UNCON', 'PV_div_AQ']\n",
            "Class distribution after resampling:\n",
            " UNACC\n",
            "0    247\n",
            "1    247\n",
            "Name: count, dtype: int64\n",
            "\n",
            "--- üöÄ Training and evaluating LightGBM ---\n",
            "Best parameters found: {'learning_rate': 0.05, 'max_depth': 10, 'num_leaves': 20, 'reg_lambda': 0.1}\n",
            "Calibrating model probabilities...\n",
            "Optimal threshold found: 0.10\n",
            "\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "    Responsive (0)       0.89      0.46      0.61       124\n",
            "Non-responsive (1)       0.14      0.61      0.23        18\n",
            "\n",
            "          accuracy                           0.48       142\n",
            "         macro avg       0.52      0.54      0.42       142\n",
            "      weighted avg       0.80      0.48      0.56       142\n",
            "\n",
            "\n",
            "--- üöÄ Training and evaluating CatBoost ---\n",
            "Best parameters found: {'depth': 6, 'iterations': 1000, 'l2_leaf_reg': 3, 'learning_rate': 0.01}\n",
            "Calibrating model probabilities...\n",
            "Optimal threshold found: 0.10\n",
            "\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "    Responsive (0)       0.88      0.48      0.62       124\n",
            "Non-responsive (1)       0.14      0.56      0.22        18\n",
            "\n",
            "          accuracy                           0.49       142\n",
            "         macro avg       0.51      0.52      0.42       142\n",
            "      weighted avg       0.79      0.49      0.57       142\n",
            "\n",
            "\n",
            "--- üöÄ Training and evaluating RandomForest ---\n",
            "Best parameters found: {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 2, 'n_estimators': 300}\n",
            "Calibrating model probabilities...\n",
            "Optimal threshold found: 0.15\n",
            "\n",
            "Classification Report:\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "    Responsive (0)       0.90      0.52      0.66       124\n",
            "Non-responsive (1)       0.16      0.61      0.25        18\n",
            "\n",
            "          accuracy                           0.54       142\n",
            "         macro avg       0.53      0.57      0.46       142\n",
            "      weighted avg       0.81      0.54      0.61       142\n",
            "\n",
            "\n",
            "\n",
            "--- üìä Final Model Performance Summary ---\n",
            "       Model  Accuracy  Recall (Class 1)  F1-Score (Class 1)  Precision (Class 1)   PR-AUC  ROC-AUC  False Negatives\n",
            "RandomForest  0.535211          0.611111            0.250000             0.157143 0.170071 0.590054                7\n",
            "    LightGBM  0.478873          0.611111            0.229167             0.141026 0.144125 0.546595                7\n",
            "    CatBoost  0.492958          0.555556            0.217391             0.135135 0.146077 0.524642                8\n",
            "\n",
            "‚úÖ Final summary saved to 'model_outputs_with_final_plots/model_performance_summary.csv'\n",
            "üìä Generating combined ROC curve plot...\n",
            "‚úÖ Combined ROC curve plot saved.\n",
            "\n",
            "üî¨ Generating a specific SHAP Waterfall plot for RandomForest...\n",
            "‚úÖ Saved specific RandomForest SHAP plot to its directory.\n",
            "\n",
            "--- Script finished! ---\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tempfile\n",
        "import warnings\n",
        "import logging\n",
        "from scipy.stats import mstats\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, roc_auc_score,\n",
        "    precision_recall_curve, f1_score, recall_score, auc, make_scorer,\n",
        "    precision_score, roc_curve\n",
        ")\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_selection import RFECV\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "from lightgbm import LGBMClassifier, early_stopping\n",
        "from catboost import CatBoostClassifier\n",
        "import shap\n",
        "\n",
        "## --- Section 0: Initial Setup ---\n",
        "logging.basicConfig(filename='model_training.log', level=logging.ERROR,\n",
        "                    format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "os.environ[\"CATBOOST_TEMP_DIR\"] = temp_dir\n",
        "\n",
        "## --- Section 1: Load and Prepare Data ---\n",
        "try:\n",
        "    file_path = \"thesis dataset new.xlsx\" # Assumes file is uploaded to Colab session\n",
        "    df = pd.read_excel(file_path)\n",
        "    print(\"‚úÖ Dataset loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    error_msg = \"Dataset file not found. Please upload the file to your Colab session and update the path.\"\n",
        "    logging.error(error_msg)\n",
        "    print(f\"‚ùå ERROR: {error_msg}\")\n",
        "    exit()\n",
        "\n",
        "output_dir = \"model_outputs_with_final_plots\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "df = df.dropna()\n",
        "TARGET = 'UNACC'\n",
        "\n",
        "## --- Section 2: Advanced Feature Engineering ---\n",
        "main_features_list = ['CON', 'UNCON', 'EP', 'PV', 'AQ', 'EL', 'SIZE', 'INST', 'LEV', 'IC']\n",
        "\n",
        "for col in ['CON', 'UNCON', 'EL', 'LEV', 'SIZE', 'EP', 'AQ', 'PV']:\n",
        "    if df[col].min() <= 0:\n",
        "        df[col] = df[col] - df[col].min() + 1e-6\n",
        "    df[col] = np.log1p(df[col])\n",
        "    df[col] = mstats.winsorize(df[col], limits=[0.05, 0.05])\n",
        "\n",
        "print(\"Applying Target Encoding for 'BC' feature...\")\n",
        "bc_target_map = df.groupby('BC')[TARGET].mean()\n",
        "df['BC_target_encoded'] = df['BC'].map(bc_target_map)\n",
        "df = df.drop('BC', axis=1)\n",
        "main_features_list.append('BC_target_encoded')\n",
        "\n",
        "X_pre_scaling = df.drop(TARGET, axis=1).select_dtypes(include=np.number).copy()\n",
        "y = df[TARGET]\n",
        "\n",
        "X_pre_scaling['CON_EP_interaction'] = X_pre_scaling['CON'] * X_pre_scaling['EP']\n",
        "X_pre_scaling['IC_CON_interaction'] = X_pre_scaling['IC'] * X_pre_scaling['CON']\n",
        "X_pre_scaling['SIZE_LEV_interaction'] = X_pre_scaling['SIZE'] * X_pre_scaling['LEV']\n",
        "X_pre_scaling['INST_IC_interaction'] = X_pre_scaling['INST'] * X_pre_scaling['IC']\n",
        "\n",
        "print(\"Creating new ratio features...\")\n",
        "epsilon = 1e-6\n",
        "X_pre_scaling['CON_div_SIZE'] = X_pre_scaling['CON'] / (X_pre_scaling['SIZE'] + epsilon)\n",
        "X_pre_scaling['LEV_div_SIZE'] = X_pre_scaling['LEV'] / (X_pre_scaling['SIZE'] + epsilon)\n",
        "X_pre_scaling['INST_div_SIZE'] = X_pre_scaling['INST'] / (X_pre_scaling['SIZE'] + epsilon)\n",
        "X_pre_scaling['EP_div_AQ'] = X_pre_scaling['EP'] / (X_pre_scaling['AQ'] + epsilon)\n",
        "X_pre_scaling['CON_div_UNCON'] = X_pre_scaling['CON'] / (X_pre_scaling['UNCON'] + epsilon)\n",
        "X_pre_scaling['PV_div_AQ'] = X_pre_scaling['PV'] / (X_pre_scaling['AQ'] + epsilon)\n",
        "\n",
        "X_pre_scaling.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "X_pre_scaling.fillna(0, inplace=True)\n",
        "print(\"‚úÖ Advanced feature engineering complete.\")\n",
        "\n",
        "print(\"üìä Generating correlation plot for main features against the target...\")\n",
        "main_df_corr = pd.concat([X_pre_scaling[main_features_list], y], axis=1)\n",
        "plt.figure(figsize=(10, 8))\n",
        "main_df_corr.corr()[TARGET].drop(TARGET).sort_values(ascending=False).plot(kind='bar', color='skyblue')\n",
        "plt.title(f'Correlation of Main Features with Target ({TARGET})', fontsize=16)\n",
        "plt.ylabel('Correlation Coefficient')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(output_dir, \"main_features_correlation.png\"), dpi=300)\n",
        "plt.close()\n",
        "\n",
        "## --- Section 3: Scale Features ---\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X_pre_scaling)\n",
        "X = pd.DataFrame(X_scaled, columns=X_pre_scaling.columns)\n",
        "print(\"‚úÖ Features scaled using RobustScaler.\")\n",
        "\n",
        "## --- Section 4: Split Data ---\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "## --- Section 5: Feature Selection with RFECV ---\n",
        "print(\"üöÄ Starting feature selection with RFECV on new features...\")\n",
        "rfe_estimator = LGBMClassifier(random_state=42, verbose=-1)\n",
        "rfecv = RFECV(\n",
        "    estimator=rfe_estimator, step=1, cv=StratifiedKFold(3),\n",
        "    scoring='roc_auc', min_features_to_select=10, n_jobs=-1\n",
        ")\n",
        "rfecv.fit(X_train, y_train)\n",
        "selected_features = X_train.columns[rfecv.support_].tolist()\n",
        "print(f\"‚úÖ RFECV selected {len(selected_features)} optimal features:\\n\", selected_features)\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "## --- Section 6: Resampling Pipeline ---\n",
        "resampling_pipeline = ImbPipeline([\n",
        "    ('smote', SMOTE(sampling_strategy=0.5, random_state=42)),\n",
        "    ('under', RandomUnderSampler(sampling_strategy=1.0, random_state=42))\n",
        "])\n",
        "X_train_resampled, y_train_resampled = resampling_pipeline.fit_resample(X_train_selected, y_train)\n",
        "print(\"Class distribution after resampling:\\n\", y_train_resampled.value_counts())\n",
        "\n",
        "## --- Section 7: Custom Scorer ---\n",
        "def combined_score(y_true, y_pred):\n",
        "    recall = recall_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
        "    f1 = f1_score(y_true, y_pred, pos_label=1, zero_division=0)\n",
        "    return 0.60 * recall + 0.40 * f1\n",
        "combined_scorer = make_scorer(combined_score, greater_is_better=True)\n",
        "\n",
        "## --- Section 8: Models and Grids ---\n",
        "models = {\n",
        "    'LightGBM': {\n",
        "        'base_model': LGBMClassifier(n_estimators=2000, random_state=42, verbose=-1),\n",
        "        'params': { 'learning_rate': [0.01, 0.05], 'max_depth': [7, 10], 'reg_lambda': [0.1, 1.0], 'num_leaves': [20, 31, 40] }\n",
        "    },\n",
        "    'CatBoost': {\n",
        "        'base_model': CatBoostClassifier(random_state=42, verbose=0),\n",
        "         'params': { 'learning_rate': [0.01, 0.05], 'depth': [6, 8], 'l2_leaf_reg': [1, 3], 'iterations': [1000, 2000] }\n",
        "    },\n",
        "    'RandomForest': {\n",
        "        'base_model': RandomForestClassifier(random_state=42),\n",
        "        'params': { 'n_estimators': [300, 500], 'max_depth': [10, None], 'min_samples_leaf': [2, 4], 'criterion': ['entropy'] }\n",
        "    }\n",
        "}\n",
        "\n",
        "## --- Section 9: Train, Calibrate, and Evaluate ---\n",
        "best_models = {}\n",
        "all_results = []\n",
        "best_raw_models = {}\n",
        "all_predictions = {}\n",
        "\n",
        "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
        "    X_train_resampled, y_train_resampled, test_size=0.2, random_state=42, stratify=y_train_resampled)\n",
        "\n",
        "for name, config in models.items():\n",
        "    try:\n",
        "        print(f\"\\n--- üöÄ Training and evaluating {name} ---\")\n",
        "        grid_search = GridSearchCV(\n",
        "            config['base_model'], config['params'],\n",
        "            cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),\n",
        "            scoring=combined_scorer, n_jobs=-1, verbose=0\n",
        "        )\n",
        "        fit_params = {}\n",
        "        if name == 'LightGBM':\n",
        "            fit_params['eval_set'] = [(X_val_split, y_val_split)]\n",
        "            fit_params['callbacks'] = [early_stopping(100, verbose=False)]\n",
        "        elif name == 'CatBoost':\n",
        "            fit_params['eval_set'] = [(X_val_split, y_val_split)]\n",
        "            grid_search.estimator.set_params(early_stopping_rounds=100)\n",
        "\n",
        "        grid_search.fit(X_train_split, y_train_split, **fit_params)\n",
        "\n",
        "        best_raw_model = grid_search.best_estimator_\n",
        "        best_raw_models[name] = best_raw_model\n",
        "        print(f\"Best parameters found: {grid_search.best_params_}\")\n",
        "\n",
        "        print(\"Calibrating model probabilities...\")\n",
        "        calibrated_model = CalibratedClassifierCV(best_raw_model, method='isotonic', cv=3, n_jobs=-1)\n",
        "        calibrated_model.fit(X_train_resampled, y_train_resampled)\n",
        "        best_models[name] = calibrated_model\n",
        "\n",
        "        y_pred_prob = calibrated_model.predict_proba(X_test_selected)[:, 1]\n",
        "\n",
        "        thresholds = np.arange(0.1, 0.91, 0.05)\n",
        "        scores = [combined_score(y_test, (y_pred_prob >= t).astype(int)) for t in thresholds]\n",
        "        optimal_threshold = thresholds[np.argmax(scores)]\n",
        "        y_pred = (y_pred_prob >= optimal_threshold).astype(int)\n",
        "        all_predictions[name] = y_pred\n",
        "        print(f\"Optimal threshold found: {optimal_threshold:.2f}\")\n",
        "\n",
        "        cm = confusion_matrix(y_test, y_pred)\n",
        "        precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n",
        "        results = {'Model': name, 'Accuracy': (y_test == y_pred).mean(), 'Recall (Class 1)': recall_score(y_test, y_pred, pos_label=1), 'F1-Score (Class 1)': f1_score(y_test, y_pred, pos_label=1), 'Precision (Class 1)': precision_score(y_test, y_pred, pos_label=1, zero_division=0), 'PR-AUC': auc(recall, precision), 'ROC-AUC': roc_auc_score(y_test, y_pred_prob), 'False Negatives': cm[1, 0]}\n",
        "        all_results.append(results)\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred, target_names=['Responsive (0)', 'Non-responsive (1)']))\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR during {name} training: {e}\")\n",
        "        continue\n",
        "\n",
        "## --- Section 10: Final Results Summary and Plots ---\n",
        "if all_results:\n",
        "    results_df = pd.DataFrame(all_results)\n",
        "    results_df_sorted = results_df.sort_values(by='F1-Score (Class 1)', ascending=False)\n",
        "    print(\"\\n\\n--- üìä Final Model Performance Summary ---\")\n",
        "    print(results_df_sorted.to_string(index=False))\n",
        "    results_df_sorted.to_csv(os.path.join(output_dir, \"model_performance_summary.csv\"), index=False)\n",
        "    print(f\"\\n‚úÖ Final summary saved to '{output_dir}/model_performance_summary.csv'\")\n",
        "\n",
        "    print(\"üìä Generating combined ROC curve plot...\")\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    for name, model in best_models.items():\n",
        "        if name in results_df['Model'].values:\n",
        "            y_pred_prob = model.predict_proba(X_test_selected)[:, 1]\n",
        "            fpr, tpr, _ = roc_curve(y_test, y_pred_prob)\n",
        "            roc_auc = results_df.loc[results_df['Model'] == name, 'ROC-AUC'].iloc[0]\n",
        "            plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], 'k--', label='Random Chance')\n",
        "    plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate'); plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curves'); plt.legend(loc=\"lower right\"); plt.grid(True)\n",
        "    plt.savefig(os.path.join(output_dir, \"combined_roc_curve.png\"), dpi=300)\n",
        "    plt.close()\n",
        "    print(\"‚úÖ Combined ROC curve plot saved.\")\n",
        "else:\n",
        "    print(\"\\nNo models were successfully trained.\")\n",
        "\n",
        "# --- MODIFIED PLOT: DEDICATED SHAP WATERFALL FOR RANDOMFOREST ---\n",
        "if 'RandomForest' in best_raw_models:\n",
        "    print(\"\\nüî¨ Generating a specific SHAP Waterfall plot for RandomForest...\")\n",
        "    rf_model_output_dir = os.path.join(output_dir, 'RandomForest')\n",
        "    os.makedirs(rf_model_output_dir, exist_ok=True)\n",
        "\n",
        "    rf_raw_model = best_raw_models['RandomForest']\n",
        "    rf_predictions = all_predictions['RandomForest']\n",
        "\n",
        "    # Find a True Positive to explain (a correctly identified non-responsive company)\n",
        "    true_positives = X_test_selected[(rf_predictions == 1) & (y_test == 1)]\n",
        "\n",
        "    if not true_positives.empty:\n",
        "        sample_to_explain = true_positives.head(1)\n",
        "        plot_title = f'SHAP Explanation for a True Positive - RandomForest'\n",
        "    else:\n",
        "        sample_to_explain = X_test_selected.head(1) # Fallback to the first sample\n",
        "        plot_title = 'SHAP Local Explanation (Fallback Sample) - RandomForest'\n",
        "\n",
        "    # Explain using the raw model\n",
        "    explainer = shap.TreeExplainer(rf_raw_model)\n",
        "    # Generate an Explanation object for the specific sample\n",
        "    shap_explanation_object = explainer(sample_to_explain)\n",
        "\n",
        "    # The waterfall plot needs a 1D explanation (for a single row, single class)\n",
        "    # We select the first sample ([0]) and the values for the positive class ([:, 1])\n",
        "    waterfall_explanation_for_class1 = shap_explanation_object[0, :, 1]\n",
        "\n",
        "    # Generate and save the waterfall plot\n",
        "    plt.figure()\n",
        "    shap.waterfall_plot(waterfall_explanation_for_class1, show=False, max_display=15)\n",
        "    plt.title(plot_title, fontsize=12)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(rf_model_output_dir, \"shap_local_waterfall_true_positive.png\"), dpi=300)\n",
        "    plt.close()\n",
        "    print(f\"‚úÖ Saved specific RandomForest SHAP plot to its directory.\")\n",
        "# --- END MODIFIED PLOT ---\n",
        "\n",
        "print(\"\\n--- Script finished! ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-XJlCKePcbUM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}